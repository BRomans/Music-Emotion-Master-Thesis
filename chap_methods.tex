\chapter{Methods}
\label{chap:methods}
This chapter describes the methods used to carry on the research from the data collection to the results of the analysis. It includes a brief description of the population that took part into the experiment, the experimental task and conditions, the procedure adopted, and the equipment used. Then it delves into the analysis process starting with the preparation and preprocessing of the data and concluding with the intermediate outcomes of the classification. The experiment is the result of a collaboration between the University of Twente, that provided the space, the recruitment system and part of the recording equipment, and myBrainTechnologies that provided their EEG-capable Melomind headsets and the rest of the recording equipment.

\section{Experiment}
\label{sec:experiment}
\subsection{Experimental Annotation app for data collection}
\label{sec:experimental_annotation_app}
An app was developed to collect continuous annotations of perceived emotion, inspired by the design of the FEELTRACE tool \cite{cowie_feeltrace_2000} and the app developed by Thammasan et al. \cite{thammasan_continuous_2016}. The \ac{EA} app was developed in Python using the Psychopy\footnote{https://www.psychopy.org/}  engine for experimental behavioral sciences. The app is a collection of timed routines that alternate guided instructions, emotion annotation tasks on a simplified GUI representing valence-arousal space and forms to report familiarity/liking scores. Three training sessions have been included:
\begin{itemize}
\item T1: the participant is presented with some background information about the valence-arousal model and how to use the annotation tool.
\item T2: the participant is asked to annotate on the \ac{VA} space the perceived emotion while listening to 2 minutes of mixed music genres.
\item T3: the participant is presented with the simulation of a trial of the experiment, including reporting of familiarity/liking and the two listening conditions (see Chapter \ref{sec:task}).
\end{itemize}

The \ac{EA} app was designed and developed at myBrainTechnologies and then tested with the other employees during a short pilot period to adjust the instructions, the clarity of the GUI and the input method. Two input methods were evaluated with A/B testing methodology, using mouse and joystick respectively. The results of the test (see Appendix \ref{app:appendix_A1}) confirmed that using mouse as input source required less training and effort, thus softening the cognitive load of the annotation task while music listening. Using the joystick would have enabled collecting annotation even in an eyes-closed listening condition thanks to the tactile feedback, but at the cost of requiring more training and concentration. To record experimental timed events, the \ac{EA} app was connected to the Melomind through a TriggerBox with an USB cable, a customized Arduino Nano board that can send binary-encoded labels using the serial port. 

\subsection{Participants}
\label{sec:participants}
In respect with the Covid-19 safety measures enforced by University of Twente, 45 healthy participants (28 females) participated in the experiment, all students, or ex-students of the university. The mean age of the population is 23.8 ± 3.1, with the oldest student being 31 years old and the youngest student being 18 years old at the time of the experiment (see Appendix \ref{app:appendix_A2.1}). The lowest educational level was the enrollment as bachelor student and the highest educational level was having completed a master’s degree. Almost half of the participants (20) were Dutch, while all the rest came from different countries, but all of them had at least a C1 or equivalent English proficiency as requirement to enroll in University of Twente. Prior to be confirmed as participants, they were invited through an invitation form informing them on the nature of the experiment and collecting personal information such as demographic information, health conditions, drugs consumption, musical literacy, and some behavioral information on their habits in listening and searching for music to later support the design of a prototype. Almost 5\% of the participants reported to be left-handed, but none asked for an inverted setup of the equipment after it was offered to them (see Appendix \ref{app:appendix_A2.1}). The only strict criterion to participate in the experiment was the capability to hear music, eventually through a hearing support system. None of the applicants was discarded nor required additional support for their health conditions. Prior to their experimental session, they were asked to refrain from consuming recreational drugs and alcohol in the 12 hours before the experiment and caffeine and tobacco in the hour before the experiment to prevent induced biases in the cerebral activity.

\subsection{Stimuli selection}
\label{sec:stimuli}
The stimuli were selected to represent an even as possible distribution of emotions according to the 4 classes identifiable by the quadrants of the Valence-Arousal space. These 4 classes are the possible combinations of positive and negative valence with high and low arousal. To keep consistency with related studies \cite{koelstra_deap_2012}, the classes have been named as follows going clockwise from the top-right quadrant:
\begin{itemize}
\item HAHV: High-Arousal and High-Valence.
\item LAHV: Low-Arousal and High-Valence.
\item LALV: Low-Arousal and Low-Valence.
\item HALV: High-Arousal and Low-Valence.
\end{itemize}
Selecting the right stimuli is a non-trivial task especially in the case of music since many factors can bias the personal perception. For example, familiarity with a certain song might elicit stronger emotions or create an effect of anticipation \cite{sangnark_revealing_2021}, \cite{ward_same_2013}, \cite{salimpoor_anatomically_2011}, while cultural biases or genre preference might completely change how a song is perceived \cite{chang_personalized_2017,fang_perception_2017}. Choosing to include lyrics or no lyrics may shift the attention of the listener from the meaning to the melody and vice versa. It is clearly impossible to address all the possible issues but considering the scope of this study and the research on a realistic use-case scenario, these factors were either mitigated or considered with the due precautions. 

\begin{figure}[h!]
\includegraphics[width=12cm]{img/methods/va_space_experiment.png}
\centering
\caption{The Valence-Arousal space GUI used for the training with color cues on the left, and the uncolored Valence-Arousal space GUI used for the experiment session on the right.} \label{fig_va_space_experiment}
\end{figure}

The stimuli were finally selected as a subset of 8 songs (see Appendix \ref{app:appendix_A2.2}) from the music database created by Koelstra et al. \cite{koelstra_deap_2012}, according to their emotional tagging. Koelstra et al. used the popular online music database last.fm\footnote{https://www.last.fm/}  to retrieve through their APIs 120 songs with associated music videos, emotionally labelled by thousands of users. They then screened them down 40 stimuli during a web assessment session with at least 14 volunteers for each stimulus. The 8 songs selected for this study are a randomly picked subset of those 40 stimuli whose emotional web assessment belonged to the same Valence-Arousal quadrant as the last.fm tagging. For each quadrant there are exactly 2 songs and in total 8 emotions are supposedly portrayed: excitement, happiness, satisfaction, relaxation, depression, sadness, anger, and anxiety. 
It is important to point out that the web assessment conducted by Koelstra et al. was done using the music videos of these songs, and that the placement of the emotions in the \ac{VA} space used in this experiment (see Figure \ref{fig_va_space_experiment}) is just a functional and necessary simplification of the model presented by Russell \cite{russell_circumplex_1980}.

\subsection{Conditions}
\label{sec:conditions}
There is no common agreement in the academic world on which should be the best recording condition for an \ac{EEG} experiment, but most researchers agree on the minimization of external stimuli. Only few studies tried to assess the impact of recording in \ac{EO} condition and \ac{EC} condition on emotion analysis. Barry et al. reported \cite{barry_eeg_2007} differences in topography and power levels, due to the processing of visual input, and recommends considering them when choosing baseline conditions. Chang et al.\cite{chang_experiencing_2015} analyzed recording conditions in relation to music listening and reported that frontal theta power significantly increased in the EC condition, while asymmetries indices in the alpha power on parietal and temporal sites reflected emotional valence for \ac{EC} and \ac{EO} states respectively. In addition, participants rated music as more pleasant and more positive while listening with their eyes closed. These differences in the listening conditions did not seem to significantly impact on the current study that only used frontal electrodes but were considered in the design of the experimental task and in the choice of the resting state baseline. Another problem is caused by ocular movements generates large artifacts in the EEG signal \cite{hagemann_effects_2001}, consequently yielding lower quality data and more computationally expensive preprocessing. In the worst cases, some data must be pruned or reconstructed, varying from a few channels to the entire dataset of a participant. Eye artifacts are typically found in the electrodes placed on the frontal area of the scalp and are usually filtered away by subtracting \ac{EOG} from the \ac{EEG} signal, however it is not the case of this study that could not take advantage of extra sensors to record \ac{EOG} . In general, we can assume that an \ac{EC} condition yields better quality data than an \ac{EO} condition because the quantity of ocular artifacts will be reduced to the minimum and there is no underlying visual stimuli processing. The downsides of experimenting in the \ac{EC} condition are the obvious limitations on the task that could be presented to the participants and a possible increase of power in the Alpha band of the spectrum, that is usually amplified during resting and focused states. The main advantage of the \ac{EO} condition is the possibility to ask the participants for more complex tasks, at the cost of generating more ocular and muscular artifacts and eventually introducing multiple cognitive tasks at once, that can affect the analysis. Prior to the experiment, we run an internal pilot test at myBrainTechnologies to explore the best compromise options between having good quality data, the maximum amount of data and collecting the behavioral data we needed.
Thammasan et al. \cite{thammasan_continuous_2016} opted for a double listening protocol, in \ac{EC} condition to record \ac{EEG} and in \ac{EO} condition to collect affective annotations. This translates into listening twice to the same song but recording only in the \ac{EC} closed condition and then overlapping the annotations taken during the \ac{EO} conditions. Our limitation of 2 frontal electrodes already constrained the collectable amount of data, so we decided to extend this protocol with a double listening and recording approach, in both conditions. During the pilot we explored the feasibility of collecting annotations in both conditions using a joystick, but then opted for collecting annotations only during \ac{EO} condition with a mouse and then reuse the same annotations on the \ac{EEG} data collected in \ac{EC} condition.  As final consideration, the two conditions can be both present in realistic scenarios, with \ac{EO} being the most common listening condition, for example in an office or free-time scenario in which a user listen to music while performing other cognitive tasks (work, homework, gaming…). Listening in \ac{EC} condition resembles a more relaxed scenario, for example when listening to music at the opera or on a comfortable couch in the evening.

\subsection{Task}
\label{sec:task}
During the experimental session, participants were presented a main task during which their physiological signal were recorded. The task is divided into 3 sub-tasks for both conditions during each trial with a total of 8 trials, resulting in approximately 35 minutes of recording. The sub-tasks are defined as follows:

\begin{itemize}
\item Listening to white noise: before presenting the stimulus, participants listened to 15 seconds of white noise to “reset” their emotional state.
\item Listening to the stimulus in EO/EC conditions: participants listened to 60 seconds excerpts of each song. During the \ac{EO} condition participants were requested to continuously annotate their emotions on the \ac{VA} space, during the \ac{EC} condition they focused solely on the music. The order of presentation of the conditions depended on the assigned group.
\item Rating the stimulus: after listening to the song, participants were requested to give a rating in terms of familiarity and liking of the excerpts, using Likert scales ranging from 0 to 5. If for any reason they failed to give a score before the 20 seconds timer expired, the score would be automatically set to 3.
\end{itemize}
So, during each trial, the participants would listen to two different songs, rate them and only annotate during the \ac{EO} condition (see Fig. \ref{fig_experimental_trial}). 

\begin{figure}[h!]
\includegraphics[width=12cm]{img/methods/experimental_trial.png}
\centering
\caption{Diagram of an experimental trial starting with EO listening condition.} \label{fig_experimental_trial}
\end{figure}

It is important to underline that the order of the conditions might induce some bias in the annotation task and the rating of each stimulus. To minimize the statistical effects, participants were randomly assigned to two groups in equal distribution, ECEO and EOEC, according to an inversion criterion that would determine the order in which the conditions were presented during the entire session. This solution seemed more elegant and less confusing than fully randomizing the order of the conditions for each trial, possibly creating confusion in the instructions. In addition, instead to presenting the same song consecutively in both conditions, we decided to split the session into two parts in which the stimuli are presented in pseudo-random class order in the first part, and then inverted in the second part as shown in Figure \ref{fig_inversion_criterion}.

\begin{figure}[h!]
\includegraphics[width=12cm]{img/methods/inversion_criterion.png}
\centering
\caption{Pseudo randomization scheme. Participants listen to all songs twice, in both conditions, during the two parts of the experiment.} \label{fig_inversion_criterion}
\end{figure}

This approach also reduces the familiarity effect caused by listening twice to the same song in a short span of time and mitigates cases of extreme emotional fluctuation within each trial, for example if a very sad song would be followed by a very happy song. Between each trial this emotional fluctuation phenomenon cannot be fully eliminated, but it is statistically balanced by the pseudo-randomization of the order in which the classes are presented. 

\subsection{Equipment}
\label{sec:equipment}
During the pilot test some technical issues with the Melomind Q+ forced the use the standard Melomind (Fig. \ref{fig_melomind}) in frontal setup using electrodes placed over [AF3 AF4] positions of the 10-20 system, thus reducing the amount of recording electrodes from 4 to 2. The Melomind was connected via USB cable to a laptop through a TriggerBox and via Bluetooth to a smartphone to remotely control the start and end of the acquisition using the proprietary Acquisier app developed by myBrainTechnologies. An EmpaticaE4  wristband was used to collect bio signals from the non-dominant hand of the participant, in particular \ac{BVP}, body temperature, \ac{HR} and \ac{EDA} (Fig. \ref{fig_empatica}). 

\begin{figure}[h!]
\includegraphics[width=8cm]{img/methods/empaticaE4.png}
\centering
\caption{EmpaticaE4, a wearable device that can record physiological data in real-time.} \label{fig_empatica}
\end{figure}

The Empatica E4 was connected to an Android tablet running the Empatica application, so the researcher could monitor in real-time the data collection. The \ac{EA} app was entirely developed as a set of automated routines in Psychopy, including trainings for the task, synchronization of the triggers with each experimental event and instructions for the users. The timers of each routine were calibrated during the pilot to allow even slower readers to follow up. The participants could interact with the experimental application through an external monitor connected to the researcher’s laptop and an agnostic mouse, although all participants decided to use the right hand. Finally, all the sessions were recorded with a GoPro Hero 7 to monitor accidental events and eventually support the emotion recognition task through facial expressions in a later study (Fig. 21). 

\begin{figure}[h!]
\includegraphics[width=14cm]{img/methods/experimental_setup.png}
\centering
\caption{Experimental setup with Melomind, EmpaticaE4 and GoProH7. The EA app is running on the monitor, while a participant is annotating emotions using the mouse (on the left) and then following instructions for the next task (on the right).} \label{fig_experimental_setup}
\end{figure}

\subsection{Procedure}
\label{sec:procedure}
The experiment was conducted in a controlled environment made available by BMS lab at the University of Twente, with a strict protocol of sanitization of the equipment between each session, no direct skin-contact with the researcher during the setup, opening of the airflows every 10-15 minutes and at least 1.5 meters of distance with the researcher during the experimental task.

\begin{figure}[h!]
\includegraphics[width=10cm]{img/methods/exp_procedure.png}
\centering
\caption{Scheme of the experimental procedure, estimated to last ~70 minutes.} \label{fig_exp_procedure}
\end{figure}

Participants were invited to sanitize their hands, to read and sign the informed consent form and to then to fill a PANAS questionnaire \cite{watson_development_nodate} as part of myBrainTechnologies standard protocol for experimentation. The PANAS is used to measure the change in positive and negative affects in a specific span of time, from a few minutes up to a few weeks. In this case the span of time is the length of the experiment and researchers at myBrainTechnologies use these questionnaires to evaluate their experimental protocols and eventually support some evidence in their analyses. After the questionnaire, they could start the training session divided into three parts for a total of 10 minutes, without recording any \ac{EEG}. The first part gave some introduction and background about the Valence-Arousal model and allowed them to get some confidence with the annotation GUI. The second part proposed a mix of 4 music excerpts, one for each Valence-Arousal class, and asked them to annotate in real time. Finally, the third part was a complete simulation of an experimental trial, including instructions, white noises, both listening conditions and ratings. After the training, participants were asked to fit again the device on their head, then the researcher re-positioned the reference and recording electrodes to obtain the best possible quality signal using the Quality Checker tool of the Acquisier app. The EmpaticaE4 was then fit on their wrist to allow a precise calculation of heart rate, and finally the Melomind was connected to the laptop through the TriggerBox. Participants were also advised to avoid sudden head movements. Before starting the session, their resting state baseline was recorded, 2 minutes in \ac{EO} condition and 2 minutes in \ac{EC} condition. When they were ready, they could start the session and follow the automated instructions, with the order of conditions determined by their assigned group. Halfway through the session they could take a 5-minute break, look away from the screen and drink some water, but they were not allowed to remove the equipment. After completing the second part of the session, another resting state was recorded with the same settings of the previous one, and then they filled the second PANAS questionnaire. The resting state recordings are also part of the standard myBrainTechnologies protocol to compare the mental changes in the resting states after an experiment and to be used as baseline during \ac{EEG} analysis. For this study only the resting state in the \ac{EC} condition prior to the experimental task was eventually used as recording baseline. At the end of the session all participants were asked to fill a feedback form to briefly evaluate the comfort of the experience, the clarity of the instructions and GUI, any difficulty in the annotation task and to report some behavioral preferences during music listening. Finally, they were debriefed on the purpose of the experiment and dismissed. The total length of the session was of 75 minutes on average, with a maximum of 90 minutes in some cases where the calibration of the equipment was not satisfactory, and the participants were then compensated for their participation.

\section{Data analysis}
\label{sec:data_analysis}
\subsection{Data preparation}
\label{sec:data_preparation}
The first step in the analysis process was to reorganize each participant’s dataset in a systematic collection that could be programmatically parsed. Due to the pseudo-randomization of the classes and the two different conditions, all trials, white noises, and resting states were flagged using an encoded label through the TriggerBox during the experimental phase. The labels were sent using timed events by the \ac{EA} app with a precision in the order of milliseconds. The resulting dictionary of events was used to split the \ac{EEG} recording and extract trials, white noises, and resting states. Each trial was associated with the appropriate label in the format condition/class\_*\_* , where condition could be a value between “EO” and “EC” to represent the recording condition, the first * could be a number in the range [1-4] to represent the valence-arousal  class, and the second * a letter between “A” or “B” to represent the order of presentation of a song within each trial. In addition, for each \ac{EEG} segment, all the \ac{QC} labels were saved for later use (see Chapter \ref{sec:automated_pipeline}). Then another dictionary containing the order of presentation of the classes was used to associate the metadata saved by the \ac{EA} app, namely the valence-arousal annotations and the familiarity/liking scores for each song, to the respective entry in the newly organized dataset. During the experiment sessions, some rare bugs in the recording application caused brief interruptions in the recording or the failure to register triggered events. For this reason, a total of 6 participants who had missing parts in their \ac{EEG} recordings or did not have the dictionary of events were excluded for further analysis. Their data could still be utilized in future studies by synchronizing the splitting functions with the timestamps saved in the metadata, but because of the time cost, it was decided to exclude them for the current research.

\subsection{Automated Pre-processing Pipeline}
\label{sec:automated_pipeline}
Given the goal of estimating the performances of a real-time oriented model, the pre-processing of the data had to consist of a lightweight and automated process that could be integrated in an application at some point. Consequently, more run-in tools like EEGLab and PREP \cite{bigdely-shamlo_prep_2015}, both based on the MATLAB programming language and very popular for offline analysis, were discarded in favor for more real-time oriented tools. Therefore, the \ac{AuPP} was implemented as a combination functions of the open-source Python library MNE\footnote{https://mne.tools/stable/index.html}  and the SignalProcessingToolbox from myBrainTechnologies, a closed-source library that is more suitable to handle the proprietary data format of Melomind. After loading each participant’s prepared dataset, the \ac{AuPP} splits the signal in time windows of 5 seconds, removes the DC offset, applies a notch filter to remove power-line noise in the 50Hz and the 100Hz frequency bands, then applies a band-pass filter in the range 0.1Hz - 30Hz to remove slow and possibly large amplitude drifts and some movement artifacts outside of the frequency bands of interest (Fig. 22). Unfortunately, this light preprocessing is not suitable to remove most of the muscular artifacts, especially those generated by ocular movements that are frequently present in the frontal electrodes. In addition, having two electrodes hinders artifact detection using more sophisticated signal processing algorithms like \ac{ICA} and \ac{PCA}, that require a higher number of electrodes to effectively separate the signal in components and identify artifacts. To deal with artifacts, the \ac{AuPP} features two methods that can be used independently or in conjunction.

\begin{figure}[h!]
\includegraphics[width=15cm]{img/methods/preprocessing_pipeline.png}
\centering
\caption{Flowchart representing the preprocessing steps of the AuPP.} \label{fig_prep_pipeline}
\end{figure}

The first one is  \ac{ASR}, available in the open-source MEEGkit\footnote{https://nbara.github.io/python-meegkit/index.html}  library, that automatically tries to clean the signal by removing transient and large-amplitude artefacts. The second one is a custom method called \ac{QIRem}, implemented for this study, and based on the\ac{QC} proprietary classification-based method developed by myBrainTechnologies. The \ac{QC} algorithm has been developed to support researchers in real-time visual assessment of the quality of the signal \cite{grosselin_quality_2019}, and for each second of recording it assigns a label representing the quality of the signal as follows:
\begin{itemize}
\item 	Low Quality: LOW-Q = -1 and 0. 
\item 	Medium Quality with muscular artefacts: MED-MUSC = 0.25. 
\item 	Medium Quality: MED-Q = 0.5. 
\item     High Quality: HIGH-Q = 1.
\end{itemize}

The \ac{QIRem} method takes the \ac{QC} labels and redistribute them on a simpler scale from 0 to 1, where 0 corresponds to LOW-Q, 0.5 corresponds to MED-MUSC-Q and MED-Q and 1 corresponds to HIGH-Q, and then for each 5 seconds time window it calculates an average of the \ac{QC} labels.

\begin{figure}[h!]
\includegraphics[width=15cm]{img/methods/qirem_example.png}
\centering
\caption{Example of how QIRem flags bad segments flagged for removal.} \label{fig_qirem_example}
\end{figure}

If the average is below a specified “threshold” parameter, the EEG split is removed from the dataset (Fig. \ref{fig_qirem_example}). After removing all the contaminated splits if the dataset has lost more the percentage of data than the amount specified by the \emph{allowed loss} parameter, the entire participant’s dataset is flagged for exclusion from the analysis. During the tuning of the \ac{AuPP}, it was finally decided to avoid using \ac{ASR} to clean the signal, because it requires to be trained on a clean segment of signal and most often resulted in very aggressive cleaning that would flatten the signal (Fig. \ref{fig_asr_example}).

\begin{figure}[h!]
\includegraphics[width=15cm]{img/methods/asr_example.png}
\centering
\caption{Examples of the effects of ASR on the signal.} \label{fig_asr_example}
\end{figure}

The \ac{QIRem} method was setup with “threshold” set to 0.5 and allowed loss set to 0.25, meaning that the average quality of each time window had to be equal or above 0.5 in the simplified scale and that at most 25\% of data could be pruned before flagging the entire dataset for exclusion. With the current configuration, 10 participants were excluded from further analysis, hence why the threshold was kept around medium quality (0.5), allowing some artefacts to persist in the data. This approach is a compromise choice that carries three main problems that must be addressed in future studies:
\begin{itemize}
\item 	Very aggressive: bad quality data are not cleaned, but removed instead, possibly losing meaningful information and control over the distribution of the class labels.
\item 	Exclusive: 10 out of 39 participants were excluded from analysis, which summed up to those excluded for other reasons is more than 1/3 of the entire experimental dataset.
\item 	Not Optimized: one limitation of the Quality Checker algorithm is that it was trained for the consumer use on Melomind with electrodes placed on the parietal area of the scalp, so while able to discriminate good and bad quality segments of signal, it has no specific label for ocular artifacts. An upgraded version is under the work to provide classification of these artifacts.
\end{itemize}

\subsection{Features Computation}
\label{sec:features_computation}
To compute the spectral features, the \ac{PSD} of each 5 seconds window of the \ac{EEG} signal was extracted and filtered for theta, alpha and beta frequency band using the Signal Processing Toolbox wrapper for the \ac{FFT}. Before computing features, the time-frequency power was normalized using the decibel conversion method as described by M. X. Cohen \cite{cohen_analyzing_2014}. Time-frequency power follows a 1/f shape function, meaning that frequency spectrum tends to show decreasing power at increasing frequencies, \ac{EEG} included. Consequently, there are 5 main limitations:

\begin{enumerate}
\item 	Difficulty in visualizing power across a large range of frequency bands
\item 	Difficulty in making power comparisons across such bands. Raw power values change in scale as a function of frequency, meaning that lower frequencies (Delta, Theta) will show larger effect than higher frequencies in terms of overall magnitude.
\item 	Aggregation of subject-independent effects will not yield good results because of differences influenced by skill thickness, sulcal anatomy, cortical surface, recording environment or other internal and external factors.
\item 	Task-related changes in power can be tainted by background activity, particularly for frequencies that tend to have higher power, especially during baseline periods (Alpha)
\item 	Raw powers do not follow a normal distribution because they cannot be negative, and they are strongly positively distorted.
\end{enumerate}
Using decibel conversion, the ratio between strength of one signal (frequency-band-specific-power) and the strength of another signal (a baseline level of power in the same frequency band).
\[dB_{tf} = 10log10 \left(\frac{activity_{tf}}{\overline{baseline_{tf}}} \right)\]
The scale and interpretation of frequency-band-specific power becomes the change in power relative to the baseline. Any frequency-band-specific activity constant over time will be removed, including background activity. As a baseline for normalization, the resting state in the \ac{EC} condition was used, to prevent ocular artifacts from contaminating the trials. The baseline resting state, previously divided in 5 seconds time window, and pre-processed together with the other trials, was pruned from low quality segments using the \ac{QIRem} function and then averaged across all the time windows, for each channel. The main advantages obtainable by normalizing the date are the following: 

\begin{itemize}
\item 	All power data are re-scaled to the same scale and thus can be compared visually and statistically
\item 	Normalization computed in respect to a pre-trial baseline enables to disentangle time-frequency dynamics from background or task-unrelated dynamics
\item 	All power results are in a common and easily numerically interpretable metrics
\item 	Parametric statistical analysis is appropriated to use (for baseline-normalized power data normally distributed) and quantitative group-level analyses and integration with other data (behavioral performance, questionnaires) is facilitated. 
\end{itemize}
For each 5s time window the following measurements were computed and stored for a total of 40 features among the two channels to be used in classification:
\begin{enumerate}
\item 	Normalized power in theta, alpha and beta frequency bands
\item 	Approach-Withdrawal Index 
\item 	Frontal-Midline Theta Index
\item 	Spectral Asymmetry Indexes
\item 	Skewness of the power in theta, alpha and beta frequency bands
\item 	Kurtosis of the power in theta, alpha and beta frequency bands
\item     Standard deviation of the power in theta, alpha and beta frequency bands
\item 	Ratio of the power in theta, alpha and beta frequency bands
\item 	Relative spectral difference of the power in theta, alpha and beta frequency bands
\end{enumerate}

These features include the neuromarkers described in Chapter \ref{chap:background} and additional properties of the power spectrum that could strengthen the models’ ability to discriminate emotional dimensions. 

\begin{figure}[h!]
\includegraphics[width=15cm]{img/methods/example_avg_annotations.png}
\centering
\caption{Example of annotations of a single participant for all trials. The labels are color coded according to the pre-labelled VA class of each song.} \label{fig_avg_annotations}
\end{figure}

Finally, the raw VA annotations (Fig. \ref{fig_avg_annotations}) were averaged for each time-window and then converted into positive labels whether the average was positive, or negative labels otherwise. Consequentially, each time window was labelled twice: first as HA or LA (High/Low Arousal), and then as HV or LV (High/Low Valence). The union of the two labels generates one of the class labels of the Valence-Arousal quadrant that represent the emotion elicited in that specific time window, according to the notation proposed by Koelstra et al. \cite{koelstra_deap_2012} (see Chapter \ref{sec:stimuli}). In some studies, the notation for valence is defined as PV and NV (Positive/Negative Valence), which is better aligned with the etymology of positive and negative emotions. The labels were also copied for each song from the \ac{EO} listening condition to the respective \ac{EC} listening condition.

\subsection{Classification}
\label{sec:classification}
The classification pipeline was implemented using the open-source Python library Scikit-Learn\footnote{https://scikit-learn.org/stable/index.html}. Multiple experiments were conducted with two supervised learning models, Support-Vector Machine and Multilayer Perceptron. These models are a popular choice for the Emotion-Recognition task thanks to their relative simplicity yet their superior capacity to handle not linearly separable data compared to statistical linear models (see Chapter \ref{sec:classification_emotions}). The SVM architecture was defined using RBF kernel, that usually grants better accuracy, and it is relatively easy to calibrate, and decision function one-vs-one for binary classification and one-vs-rest for multi-class classification. The architecture for MLP was based on the LBFGS optimizer, a quasi-Newtonian method, that is more suitable for small datasets and can converge faster and perform better and ReLU was chosen over TanH as activation function because it reduces the impact of vanishing gradients, even if no substantial difference was observed while testing both. The problem has been set as a separate binary classification of Arousal and Valence using a subject-dependent strategy, similarly to most related studies. As explained in the Section 4.2.5, during the intermediate experiments the listening condition did not reveal significant differences in the classification performances, therefore all the trials of each subject were unified under a third condition named “EO\&EC” to take advantage of the greater amount of datapoints. Then, from each subject dataset, a total of 40 previously computed features were loaded in the classification pipeline. PCA was used to identify the features contributing for the 95\% of the variability of the dataset and projecting them on a lower dimensional space, thus reducing the dimensionality to ~12 components and greatly reducing the overall computation time, often referred to as curse of dimensionality. After applying PCA, the data were split into a training dataset and a test dataset in 80:20 proportion. The training dataset was used to tune the best hyper-parameters each classifier, respectively C and Gamma for SVM and Alpha and Hidden Layer Sizes for MLP, using GridSearch with a K-Fold Cross-Validation strategy, k=5. Finally, the tuned classifiers were trained with K-Fold Cross Validation leaving-one-block-out (LOBO) for testing, and the relevant score metrics were collected.
